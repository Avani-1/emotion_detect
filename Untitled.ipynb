{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef8b0252-42bc-4d41-aaa2-36972e6cded0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.image import load_img\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6913d283-092b-4689-bb7a-0e9a6b70249d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a809a071-eb9a-48b8-8af0-a86562375714",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = 'images/train'\n",
    "TEST_DIR = 'images/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df00c528-a6f1-4c2f-a0d1-4f621f930b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createdataframe(dir):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for label in os.listdir(dir):\n",
    "        for imagename in os.listdir(os.path.join(dir,label)):\n",
    "            image_paths.append(os.path.join(dir,label,imagename))\n",
    "            labels.append(label)\n",
    "        print(label, \"completed\")\n",
    "    return image_paths,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e3da0c5-e35d-4739-8526-c3c25031808d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angry completed\n",
      "disgust completed\n",
      "fear completed\n",
      "happy completed\n",
      "neutral completed\n",
      "sad completed\n",
      "surprise completed\n"
     ]
    }
   ],
   "source": [
    "train = pd.DataFrame()\n",
    "train['image'], train['label'] = createdataframe(TRAIN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a81c1921-5769-44ad-a1fd-acdcb62882e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                image     label\n",
      "0            images/train\\angry\\0.jpg     angry\n",
      "1            images/train\\angry\\1.jpg     angry\n",
      "2           images/train\\angry\\10.jpg     angry\n",
      "3        images/train\\angry\\10002.jpg     angry\n",
      "4        images/train\\angry\\10016.jpg     angry\n",
      "...                               ...       ...\n",
      "28816  images/train\\surprise\\9969.jpg  surprise\n",
      "28817  images/train\\surprise\\9985.jpg  surprise\n",
      "28818  images/train\\surprise\\9990.jpg  surprise\n",
      "28819  images/train\\surprise\\9992.jpg  surprise\n",
      "28820  images/train\\surprise\\9996.jpg  surprise\n",
      "\n",
      "[28821 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c2cdfbd-242f-49b7-8e84-2d4e0781b16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angry completed\n",
      "disgust completed\n",
      "fear completed\n",
      "happy completed\n",
      "neutral completed\n",
      "sad completed\n",
      "surprise completed\n"
     ]
    }
   ],
   "source": [
    "test = pd.DataFrame()\n",
    "test['image'], test['label'] = createdataframe(TEST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cf2bd49-8c66-4dca-8d96-af528642d562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              image     label\n",
      "0       images/test\\angry\\10052.jpg     angry\n",
      "1       images/test\\angry\\10065.jpg     angry\n",
      "2       images/test\\angry\\10079.jpg     angry\n",
      "3       images/test\\angry\\10095.jpg     angry\n",
      "4       images/test\\angry\\10121.jpg     angry\n",
      "...                             ...       ...\n",
      "7061  images/test\\surprise\\9806.jpg  surprise\n",
      "7062  images/test\\surprise\\9830.jpg  surprise\n",
      "7063  images/test\\surprise\\9853.jpg  surprise\n",
      "7064  images/test\\surprise\\9878.jpg  surprise\n",
      "7065   images/test\\surprise\\993.jpg  surprise\n",
      "\n",
      "[7066 rows x 2 columns]\n",
      "0         images/test\\angry\\10052.jpg\n",
      "1         images/test\\angry\\10065.jpg\n",
      "2         images/test\\angry\\10079.jpg\n",
      "3         images/test\\angry\\10095.jpg\n",
      "4         images/test\\angry\\10121.jpg\n",
      "                    ...              \n",
      "7061    images/test\\surprise\\9806.jpg\n",
      "7062    images/test\\surprise\\9830.jpg\n",
      "7063    images/test\\surprise\\9853.jpg\n",
      "7064    images/test\\surprise\\9878.jpg\n",
      "7065     images/test\\surprise\\993.jpg\n",
      "Name: image, Length: 7066, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(test)\n",
    "print(test['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "609bf4a5-bbb2-46f5-ac95-db3f8c373712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57899692-fb40-4870-a38e-28d6d1068d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(images):\n",
    "    features = []\n",
    "    for image in tqdm(images):\n",
    "        img = load_img(image,grayscale =  True )\n",
    "        img = np.array(img)\n",
    "        features.append(img)\n",
    "    features = np.array(features)\n",
    "    features = features.reshape(len(features),48,48,1)\n",
    "    return features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76a1d761-d983-4758-bf6b-ff91fe8fd56e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "266d9372acb84dcea9975963480ff091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28821 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\Python310\\lib\\site-packages\\keras_preprocessing\\image\\utils.py:107: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
      "  warnings.warn('grayscale is deprecated. Please use '\n"
     ]
    }
   ],
   "source": [
    "train_features = extract_features(train['image']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "383e1ef3-053f-4af9-ba62-845e3bcecb9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4827f7cb86c446fea4a1d4e26e944725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7066 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_features = extract_features(test['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcde2fef-910c-4ed3-b68a-7be5b83fe4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_features/255.0\n",
    "x_test = test_features/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e4d2ab8-1e12-4eee-91bd-1d237003a1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36d09e0a-8301-483e-a7aa-7dbd1edafaf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelEncoder</label><div class=\"sk-toggleable__content\"><pre>LabelEncoder()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9e20fa7-f97f-4a82-ad54-c22886c37581",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = le.transform(train['label'])\n",
    "y_test = le.transform(test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb11f7ad-3980-45db-8780-64ddb101bd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train,num_classes = 7)\n",
    "y_test = to_categorical(y_test,num_classes = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08c5c51a-2f1f-4d03-838a-1dcbb21c85c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# convolutional layers\n",
    "model.add(Conv2D(128, kernel_size=(3,3), activation='relu', input_shape=(48,48,1)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(256, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(512, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(512, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "# fully connected layers\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "# output layer\n",
    "model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1920754-ef69-4226-be6d-154ccab2771d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = 'accuracy' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a50879a-cf61-4d9c-a2d1-2f40ff8a3bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "226/226 [==============================] - 1273s 6s/step - loss: 1.8245 - accuracy: 0.2433 - val_loss: 1.8095 - val_accuracy: 0.2583\n",
      "Epoch 2/100\n",
      "226/226 [==============================] - 1158s 5s/step - loss: 1.7950 - accuracy: 0.2540 - val_loss: 1.7272 - val_accuracy: 0.2900\n",
      "Epoch 3/100\n",
      "226/226 [==============================] - 1241s 5s/step - loss: 1.6965 - accuracy: 0.3149 - val_loss: 1.5544 - val_accuracy: 0.3982\n",
      "Epoch 4/100\n",
      "226/226 [==============================] - 1072s 5s/step - loss: 1.5699 - accuracy: 0.3831 - val_loss: 1.4379 - val_accuracy: 0.4452\n",
      "Epoch 5/100\n",
      "226/226 [==============================] - 1280s 6s/step - loss: 1.4871 - accuracy: 0.4228 - val_loss: 1.3997 - val_accuracy: 0.4673\n",
      "Epoch 6/100\n",
      "226/226 [==============================] - 835s 4s/step - loss: 1.4362 - accuracy: 0.4500 - val_loss: 1.3175 - val_accuracy: 0.4950\n",
      "Epoch 7/100\n",
      "226/226 [==============================] - 882s 4s/step - loss: 1.3927 - accuracy: 0.4598 - val_loss: 1.2786 - val_accuracy: 0.5045\n",
      "Epoch 8/100\n",
      "226/226 [==============================] - 921s 4s/step - loss: 1.3612 - accuracy: 0.4722 - val_loss: 1.2561 - val_accuracy: 0.5192\n",
      "Epoch 9/100\n",
      "226/226 [==============================] - 1140s 5s/step - loss: 1.3399 - accuracy: 0.4863 - val_loss: 1.2392 - val_accuracy: 0.5374\n",
      "Epoch 10/100\n",
      "226/226 [==============================] - 921s 4s/step - loss: 1.3157 - accuracy: 0.4972 - val_loss: 1.2065 - val_accuracy: 0.5444\n",
      "Epoch 11/100\n",
      "226/226 [==============================] - 924s 4s/step - loss: 1.2939 - accuracy: 0.5075 - val_loss: 1.1929 - val_accuracy: 0.5545\n",
      "Epoch 12/100\n",
      "226/226 [==============================] - 927s 4s/step - loss: 1.2792 - accuracy: 0.5117 - val_loss: 1.1730 - val_accuracy: 0.5508\n",
      "Epoch 13/100\n",
      "226/226 [==============================] - 904s 4s/step - loss: 1.2629 - accuracy: 0.5173 - val_loss: 1.1757 - val_accuracy: 0.5528\n",
      "Epoch 14/100\n",
      "226/226 [==============================] - 901s 4s/step - loss: 1.2418 - accuracy: 0.5266 - val_loss: 1.1663 - val_accuracy: 0.5654\n",
      "Epoch 15/100\n",
      "226/226 [==============================] - 875s 4s/step - loss: 1.2303 - accuracy: 0.5304 - val_loss: 1.1526 - val_accuracy: 0.5664\n",
      "Epoch 16/100\n",
      "226/226 [==============================] - 810s 4s/step - loss: 1.2207 - accuracy: 0.5350 - val_loss: 1.1402 - val_accuracy: 0.5620\n",
      "Epoch 17/100\n",
      "226/226 [==============================] - 799s 4s/step - loss: 1.2028 - accuracy: 0.5409 - val_loss: 1.1312 - val_accuracy: 0.5720\n",
      "Epoch 18/100\n",
      "226/226 [==============================] - 841s 4s/step - loss: 1.1919 - accuracy: 0.5444 - val_loss: 1.1256 - val_accuracy: 0.5760\n",
      "Epoch 19/100\n",
      "226/226 [==============================] - 796s 4s/step - loss: 1.1846 - accuracy: 0.5535 - val_loss: 1.1151 - val_accuracy: 0.5811\n",
      "Epoch 20/100\n",
      "226/226 [==============================] - 869s 4s/step - loss: 1.1714 - accuracy: 0.5556 - val_loss: 1.1108 - val_accuracy: 0.5870\n",
      "Epoch 21/100\n",
      "226/226 [==============================] - 896s 4s/step - loss: 1.1667 - accuracy: 0.5592 - val_loss: 1.0950 - val_accuracy: 0.5903\n",
      "Epoch 22/100\n",
      "172/226 [=====================>........] - ETA: 3:24 - loss: 1.1571 - accuracy: 0.5618"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \n",
      "File \u001b[1;32mC:\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mC:\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1776\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1777\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1780\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1781\u001b[0m ):\n\u001b[0;32m   1782\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1783\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1785\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mC:\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mC:\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    828\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    830\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 831\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    833\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    834\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mC:\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    864\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    865\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 867\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    872\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mC:\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1260\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1262\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1263\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1264\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1265\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1266\u001b[0m     args,\n\u001b[0;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1268\u001b[0m     executing_eagerly)\n\u001b[0;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mC:\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflat_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    216\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 217\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mC:\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    251\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 252\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    257\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    261\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    262\u001b[0m     )\n",
      "File \u001b[1;32mC:\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1477\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1479\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1480\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1481\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1482\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1483\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1484\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1485\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1487\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1488\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1489\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1493\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1494\u001b[0m   )\n",
      "File \u001b[1;32mC:\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     53\u001b[0m   \u001b[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     55\u001b[0m       tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[0;32m     56\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, core_types\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[0;32m     57\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[0;32m     58\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[0;32m     59\u001b[0m   ]\n\u001b[1;32m---> 60\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     63\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(x= x_train,y = y_train, batch_size = 128, epochs = 100, validation_data = (x_test,y_test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "313f0093-6f38-4730-a7b6-5810c7f95a6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model_json \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mto_json()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memotiondetector.json\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m json_file:\n\u001b[0;32m      3\u001b[0m     json_file\u001b[38;5;241m.\u001b[39mwrite(model_json)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"emotiondetector.json\",'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save(\"emotiondetector.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87256bdb-1b52-4535-9db2-a79d8b8bf87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f08f2876-efef-4ac3-adf7-a2e108277693",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open(\"emotiondetector.json\", \"r\")\n",
    "model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(model_json)\n",
    "model.load_weights(\"emotiondetector.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3d18744-0feb-4803-a513-ca7dd6ec6134",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ['angry','disgust','fear','happy','neutral','sad','surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56ccc1e1-3b40-4e81-bd07-a9955e00dc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ef(image):\n",
    "    img = load_img(image,grayscale =  True )\n",
    "    feature = np.array(img)\n",
    "    feature = feature.reshape(1,48,48,1)\n",
    "    return feature/255.0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ffcf269-5f85-40e1-9e11-4f440e693843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original image is of sad\n",
      "1/1 [==============================] - 1s 529ms/step\n",
      "model prediction is  sad\n"
     ]
    }
   ],
   "source": [
    "image = 'images/test/sad/231.jpg'\n",
    "print(\"original image is of sad\")\n",
    "img = ef(image)\n",
    "pred = model.predict(img)\n",
    "pred_label = label[pred.argmax()]\n",
    "print(\"model prediction is \",pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20d794a7-4bf9-4aa4-91bd-382080b9fff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original image is of happy\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "model prediction is  happy\n"
     ]
    }
   ],
   "source": [
    "image = 'images/test/happy/30.jpg'\n",
    "print(\"original image is of happy\")\n",
    "img = ef(image)\n",
    "pred = model.predict(img)\n",
    "pred_label = label[pred.argmax()]\n",
    "print(\"model prediction is \",pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28ac34dc-1c3c-43e9-af00-18c6b1a6533e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8218a72-9165-437a-a079-a01dd5204f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original image is of sad\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "model prediction is  sad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\Python310\\lib\\site-packages\\keras_preprocessing\\image\\utils.py:107: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
      "  warnings.warn('grayscale is deprecated. Please use '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c5b4ca7220>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = 'images/test/sad/231.jpg'\n",
    "print(\"original image is of sad\")\n",
    "img = ef(image)\n",
    "pred = model.predict(img)\n",
    "pred_label = label[pred.argmax()]\n",
    "print(\"model prediction is \",pred_label)\n",
    "plt.imshow(img.reshape(48,48),cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
